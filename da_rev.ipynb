{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0K9H8Nk-N_Tf",
      "metadata": {
        "id": "0K9H8Nk-N_Tf"
      },
      "source": [
        "# Domain adaptation on classification task with AlexNet and PACS dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5q3A_omGNpCf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q3A_omGNpCf",
        "outputId": "ddfb9906-dcb2-49cc-f6f6-c8b3847a2238"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GrQxrFD9QBIi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrQxrFD9QBIi",
        "outputId": "7ff45564-6cac-4adc-a23e-a16a51621b12"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/SimoneBorella/domain-adaptation.git\n",
        "\n",
        "!cp -r ./domain-adaptation/models .\n",
        "!cp -r ./domain-adaptation/utils .\n",
        "\n",
        "!rm -r ./domain-adaptation\n",
        "!rm -r sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e686f1d3-211d-4de8-9e34-520059da376f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e686f1d3-211d-4de8-9e34-520059da376f",
        "outputId": "8062df91-42b2-41f1-959a-56392fc59265"
      },
      "outputs": [],
      "source": [
        "# Download PACS Dataset Images\n",
        "!git clone https://github.com/MachineLearning2020/Homework3-PACS/\n",
        "!mkdir -p data\n",
        "!mv Homework3-PACS/PACS data/\n",
        "!rm -r Homework3-PACS/\n",
        "\n",
        "# Download PACS Dataset Labels\n",
        "!git clone https://github.com/silvia1993/DANN_Template/\n",
        "!mv DANN_Template/txt_lists/art_painting.txt data/PACS/\n",
        "!mv DANN_Template/txt_lists/cartoon.txt data/PACS/\n",
        "!mv DANN_Template/txt_lists/photo.txt data/PACS/\n",
        "!mv DANN_Template/txt_lists/sketch.txt data/PACS/\n",
        "!rm -r DANN_Template/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "vVcmrIe_TGGA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVcmrIe_TGGA",
        "outputId": "8e7ca6eb-d4c4-44dd-8dd1-62281de5d028"
      },
      "outputs": [],
      "source": [
        "# Google colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# ! mkdir -p /content/drive/MyDrive/da\n",
        "# res_dir = \"/content/drive/MyDrive/da\"\n",
        "\n",
        "\n",
        "# Local\n",
        "res_dir = \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "46c3fbc9-679c-46a3-ae6a-d2ce96d21d43",
      "metadata": {
        "id": "46c3fbc9-679c-46a3-ae6a-d2ce96d21d43"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.backends import cudnn\n",
        "import torchvision.transforms as T\n",
        "from torch.hub import load_state_dict_from_url\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from models.alexnet_dann_rev import AlexNetDANNRev\n",
        "from utils.monitor import Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2u596HRYSws5",
      "metadata": {
        "id": "2u596HRYSws5"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"CUDA available\")\n",
        "        print(f\"Number of devices: {torch.cuda.device_count()}\")\n",
        "        for dev in range(torch.cuda.device_count()):\n",
        "            print(f\"Device {dev}:\")\n",
        "            print(f\"\\tName: {torch.cuda.get_device_name(dev)}\")\n",
        "    else:\n",
        "        print(\"CUDA not available\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "mL9dN-DzSyLS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL9dN-DzSyLS",
        "outputId": "93182341-22db-4ebe-8061-e077e282d9f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA not available\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = get_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HUPEvN-ROSOw",
      "metadata": {
        "id": "HUPEvN-ROSOw"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "m_igi2pfOTvD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_igi2pfOTvD",
        "outputId": "63909502-c005-4dd6-cbc7-c613c560e5d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d8d6410f470>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SEED = 17\n",
        "\n",
        "VERSION = 1\n",
        "\n",
        "NUM_CLASSES = 7\n",
        "BATCH_SIZE = 256\n",
        "LR = 0.005           # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "EPOCHS = 30          # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "ALPHA = 0.1\n",
        "\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68dc5e59",
      "metadata": {
        "id": "68dc5e59"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "df154e98-5188-40be-8311-832f86e14788",
      "metadata": {
        "id": "df154e98-5188-40be-8311-832f86e14788"
      },
      "outputs": [],
      "source": [
        "# Define the Dataset class\n",
        "class PACSDataset(Dataset):\n",
        "    def __init__(self, domain, transform, root_dir):\n",
        "        assert domain in ['photo', 'art_painting', 'cartoon', 'sketch']\n",
        "        self.examples = [] # (img_path, class_label)\n",
        "        self.T = transform\n",
        "\n",
        "        with open(f'{root_dir}/PACS/{domain}.txt', 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip().split()\n",
        "            img_path = f\"{root_dir}/PACS/{line[0]}\"\n",
        "            class_label = int(line[1])\n",
        "            self.examples.append((img_path, class_label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path, class_label = self.examples[index]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img = self.T(img)\n",
        "        return img, class_label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1768e79",
      "metadata": {
        "id": "d1768e79"
      },
      "source": [
        "## Dataset preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4f8edb61",
      "metadata": {
        "id": "4f8edb61"
      },
      "outputs": [],
      "source": [
        "def dataset_preprocessing():\n",
        "    dataset_transform = T.Compose([\n",
        "        T.Resize(256),\n",
        "        T.CenterCrop(224),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Define the Dataset object for training & testing\n",
        "    traindataset = PACSDataset(domain='cartoon', transform=dataset_transform, root_dir='./data')\n",
        "    testdataset = PACSDataset(domain='sketch', transform=dataset_transform, root_dir='./data')\n",
        "\n",
        "    # Define the DataLoaders\n",
        "    trainloader = DataLoader(traindataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "    testloader = DataLoader(testdataset, batch_size=BATCH_SIZE, num_workers=4)\n",
        "\n",
        "    return trainloader, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "85d4f41f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85d4f41f",
        "outputId": "b2d0d257-6477-4fe1-da95-b8fc8f4c598c"
      },
      "outputs": [],
      "source": [
        "trainloader, testloader = dataset_preprocessing()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0efb1c",
      "metadata": {
        "id": "4c0efb1c"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "335d5240",
      "metadata": {
        "id": "335d5240"
      },
      "outputs": [],
      "source": [
        "def get_model(device):\n",
        "    model_urls = {\n",
        "        'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
        "    }\n",
        "\n",
        "    model = AlexNetDANNRev()\n",
        "    model.load_state_dict(load_state_dict_from_url(model_urls['alexnet'], progress=True), strict=False)\n",
        "    model.classifier[-1] = nn.Linear(4096, NUM_CLASSES)\n",
        "    model = model.to(device)\n",
        "    \n",
        "    return model\n",
        "\n",
        "def save_model(model, file_name):\n",
        "    torch.save(model.state_dict(), file_name)\n",
        "\n",
        "def load_model(model, file_name, device=\"cuda\"):\n",
        "    model.load_state_dict(torch.load(file_name, map_location=torch.device(device)))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683ceefb",
      "metadata": {
        "id": "683ceefb"
      },
      "source": [
        "## Loss function definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4dc33e58",
      "metadata": {
        "id": "4dc33e58"
      },
      "outputs": [],
      "source": [
        "def get_loss_function():\n",
        "    return nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f5a5ca",
      "metadata": {
        "id": "c0f5a5ca"
      },
      "source": [
        "## Optimizer definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b6a39fa9",
      "metadata": {
        "id": "b6a39fa9"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model):\n",
        "    return torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c94cede0",
      "metadata": {
        "id": "c94cede0"
      },
      "source": [
        "## Scheduler definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4b786dd6",
      "metadata": {
        "id": "4b786dd6"
      },
      "outputs": [],
      "source": [
        "def get_scheduler(optimizer):\n",
        "    return optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b50990",
      "metadata": {
        "id": "b8b50990"
      },
      "source": [
        "## Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "523d384d",
      "metadata": {
        "id": "523d384d"
      },
      "outputs": [],
      "source": [
        "def plot_training_metrics(train_class_losses, train_src_domain_losses, train_trg_domain_losses, learning_rates, base_dir):\n",
        "    fig = plt.figure()\n",
        "    plt.title(\"Loss\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.plot(train_class_losses, label=\"Train Class Loss\")\n",
        "    plt.plot(train_src_domain_losses, label=\"Train Src Domain Loss\")\n",
        "    plt.plot(train_trg_domain_losses, label=\"Train Trg Domain Loss\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{base_dir}/plots/loss.pdf\")\n",
        "    plt.close(fig)\n",
        "\n",
        "    fig = plt.figure()\n",
        "    plt.title(\"Learning rate\")\n",
        "    plt.ylabel(\"learning rate\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.plot(learning_rates, label=\"Learning Rate\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"{base_dir}/plots/learning_rate.pdf\")\n",
        "    plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b760999",
      "metadata": {
        "id": "6b760999"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e57392d2",
      "metadata": {
        "id": "e57392d2"
      },
      "outputs": [],
      "source": [
        "def train_dann_rev(model, trainloader, loss_function, optimizer, scheduler, device, monitor, base_dir):\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    train_class_losses = []\n",
        "    train_src_domain_losses = []\n",
        "    train_trg_domain_losses = []\n",
        "    learning_rates = []\n",
        "\n",
        "    for e in range(EPOCHS):\n",
        "        monitor.start(desc=f\"Epoch {e + 1}/{EPOCHS}\", max_progress=len(trainloader))\n",
        "\n",
        "        learning_rate = scheduler.get_last_lr()[0]\n",
        "        learning_rates.append(learning_rate)\n",
        "\n",
        "        train_class_loss = 0.0\n",
        "        train_src_domain_loss = 0.0\n",
        "        train_trg_domain_loss = 0.0\n",
        "\n",
        "        cumulative_class_loss = 0.0\n",
        "        cumulative_src_domain_loss = 0.0\n",
        "        cumulative_trg_domain_loss = 0.0\n",
        "\n",
        "        count_loss = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, ((src_inputs, src_labels), (trg_inputs, _)) in enumerate(zip(trainloader, testloader)):\n",
        "            src_inputs, src_labels = src_inputs.to(device), src_labels.to(device)\n",
        "            trg_inputs = trg_inputs.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            src_classes = model(src_inputs)\n",
        "            src_domains = model(src_inputs, alpha=ALPHA)\n",
        "            trg_domains = model(trg_inputs, alpha=ALPHA)\n",
        "            \n",
        "            \n",
        "            # Classification Loss\n",
        "            class_loss = loss_function(src_classes, src_labels)\n",
        "            cumulative_class_loss += class_loss.item()\n",
        "            class_loss.backward()\n",
        "\n",
        "            # Source Domain Adversarial Loss --> src_domains_label = 0\n",
        "            src_domains_label = torch.zeros(BATCH_SIZE, dtype=torch.int64).to(device)\n",
        "            src_domains_loss = loss_function(src_domains, src_domains_label)\n",
        "            cumulative_src_domain_loss += src_domains_loss.item()\n",
        "            src_domains_loss.backward()\n",
        "\n",
        "            # Target Domain Adversarial Loss --> trg_domains_label = 1\n",
        "            trg_domains_label = torch.ones(BATCH_SIZE, dtype=torch.int64).to(device)\n",
        "            trg_domains_loss = loss_function(trg_domains, trg_domains_label)\n",
        "            cumulative_trg_domain_loss += trg_domains_loss.item()\n",
        "            trg_domains_loss.backward()\n",
        "            \n",
        "            count_loss += 1\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_class_loss = cumulative_class_loss / count_loss\n",
        "            train_src_domain_loss = cumulative_src_domain_loss / count_loss\n",
        "            train_trg_domain_loss = cumulative_trg_domain_loss / count_loss\n",
        "            \n",
        "            monitor.update(\n",
        "                i + 1,\n",
        "                learning_rate=f\"{learning_rate:.5f}\",\n",
        "                train_class_loss=f\"{train_class_loss:.4f}\",\n",
        "                train_src_domain_loss=f\"{train_src_domain_loss:.4f}\",\n",
        "                train_trg_domain_loss=f\"{train_trg_domain_loss:.4f}\",\n",
        "            )\n",
        "\n",
        "        train_class_losses.append(train_class_loss)\n",
        "        train_src_domain_losses.append(train_src_domain_loss)\n",
        "        train_trg_domain_losses.append(train_trg_domain_loss)\n",
        "\n",
        "        monitor.stop()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        save_model(model, f\"{base_dir}/weights/last.pt\")\n",
        "\n",
        "        plot_training_metrics(\n",
        "            train_class_losses,\n",
        "            train_src_domain_losses,\n",
        "            train_trg_domain_losses,\n",
        "            learning_rates,\n",
        "            base_dir,\n",
        "        )\n",
        "\n",
        "    monitor.print_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7790a8ce",
      "metadata": {
        "id": "7790a8ce"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5f49fb07",
      "metadata": {
        "id": "5f49fb07"
      },
      "outputs": [],
      "source": [
        "def test(model, testloader, device, monitor):\n",
        "    monitor.start(desc=f\"Testing\", max_progress=len(testloader))\n",
        "\n",
        "    test_accuracy = 0.0\n",
        "    correct_predictions = 0\n",
        "    count_predictions = 0\n",
        "\n",
        "    inference_times = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(testloader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            start_time = time.perf_counter()\n",
        "            logits = model(inputs)\n",
        "            end_time = time.perf_counter()\n",
        "\n",
        "            batch_inference_time = (end_time - start_time) / inputs.size(0)\n",
        "            inference_times.append(batch_inference_time)\n",
        "\n",
        "            predicted_labels = torch.argmax(logits, dim=1)\n",
        "            count_predictions += labels.size(0)\n",
        "            correct_predictions += (predicted_labels == labels).sum().item()\n",
        "            test_accuracy = correct_predictions / count_predictions\n",
        "\n",
        "            monitor.update(\n",
        "                i + 1,\n",
        "                test_accuracy=f\"{test_accuracy:.4f}\",\n",
        "            )\n",
        "\n",
        "    monitor.stop()\n",
        "\n",
        "    mean_inference_time = np.mean(inference_times)\n",
        "    std_inference_time = np.std(inference_times)\n",
        "\n",
        "    monitor.log(f\"Accuracy on test images: {100 * test_accuracy:.3f} %\")\n",
        "    monitor.log(f\"Mean inference time: {mean_inference_time * 1000:.3f} ms\")\n",
        "    monitor.log(f\"Standard deviation of inference time: {std_inference_time * 1000:.3f} ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8957632a",
      "metadata": {
        "id": "8957632a"
      },
      "source": [
        "## DANN (Domain Adversarial Neural Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "78c59678-ac85-40f7-9ad2-71d1b9f50bc5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78c59678-ac85-40f7-9ad2-71d1b9f50bc5",
        "outputId": "8fbca1ed-831a-4d4e-c231-6d1a8c57006f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model:\n",
            "AlexNetDANNRev(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=4096, out_features=7, bias=True)\n",
            "  )\n",
            "  (domain_classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Loss function:\n",
            "CrossEntropyLoss()\n",
            "\n",
            "Optimizer:\n",
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.005\n",
            "    lr: 0.005\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 5e-05\n",
            ")\n",
            "\n",
            "Scheduler:\n",
            "StepLR\n",
            "base_lrs: [0.005]\n",
            "gamma: 0.1\n",
            "last_epoch: 0\n",
            "optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.005\n",
            "    lr: 0.005\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 5e-05\n",
            ")\n",
            "step_size: 20\n",
            "verbose: False\n",
            "\n",
            "\n",
            "Epoch 1/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:41 s \tLearning rate: 0.00500 \tTrain class loss: 1.7876 \tTrain src domain loss: 0.5135 \tTrain trg domain loss: 0.7448 \n",
            "\n",
            "Epoch 2/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:47 s \tLearning rate: 0.00500 \tTrain class loss: 0.9813 \tTrain src domain loss: 0.2743 \tTrain trg domain loss: 0.3874 \n",
            "\n",
            "Epoch 3/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:47 s \tLearning rate: 0.00500 \tTrain class loss: 0.6037 \tTrain src domain loss: 0.1194 \tTrain trg domain loss: 0.1613 \n",
            "\n",
            "Epoch 4/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:47 s \tLearning rate: 0.00500 \tTrain class loss: 0.4966 \tTrain src domain loss: 0.0878 \tTrain trg domain loss: 0.0684 \n",
            "\n",
            "Epoch 5/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:43 s \tLearning rate: 0.00500 \tTrain class loss: 0.4055 \tTrain src domain loss: 0.0852 \tTrain trg domain loss: 0.0760 \n",
            "\n",
            "Epoch 6/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:44 s \tLearning rate: 0.00500 \tTrain class loss: 0.3563 \tTrain src domain loss: 0.1219 \tTrain trg domain loss: 0.0816 \n",
            "\n",
            "Epoch 7/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:42 s \tLearning rate: 0.00500 \tTrain class loss: 0.3160 \tTrain src domain loss: 0.1101 \tTrain trg domain loss: 0.0755 \n",
            "\n",
            "Epoch 8/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:43 s \tLearning rate: 0.00500 \tTrain class loss: 0.2793 \tTrain src domain loss: 0.0917 \tTrain trg domain loss: 0.0629 \n",
            "\n",
            "Epoch 9/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:43 s \tLearning rate: 0.00500 \tTrain class loss: 0.2591 \tTrain src domain loss: 0.0613 \tTrain trg domain loss: 0.0544 \n",
            "\n",
            "Epoch 10/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:44 s \tLearning rate: 0.00500 \tTrain class loss: 0.2262 \tTrain src domain loss: 0.0795 \tTrain trg domain loss: 0.0296 \n",
            "\n",
            "Epoch 11/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:42 s \tLearning rate: 0.00500 \tTrain class loss: 0.2184 \tTrain src domain loss: 0.0559 \tTrain trg domain loss: 0.0393 \n",
            "\n",
            "Epoch 12/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:42 s \tLearning rate: 0.00500 \tTrain class loss: 0.1941 \tTrain src domain loss: 0.0413 \tTrain trg domain loss: 0.0370 \n",
            "\n",
            "Epoch 13/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:44 s \tLearning rate: 0.00500 \tTrain class loss: 0.1543 \tTrain src domain loss: 0.0452 \tTrain trg domain loss: 0.0276 \n",
            "\n",
            "Epoch 14/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:43 s \tLearning rate: 0.00500 \tTrain class loss: 0.1464 \tTrain src domain loss: 0.0397 \tTrain trg domain loss: 0.0273 \n",
            "\n",
            "Epoch 15/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:43 s \tLearning rate: 0.00500 \tTrain class loss: 0.1272 \tTrain src domain loss: 0.0338 \tTrain trg domain loss: 0.0276 \n",
            "\n",
            "Epoch 16/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:43 s \tLearning rate: 0.00500 \tTrain class loss: 0.1206 \tTrain src domain loss: 0.0396 \tTrain trg domain loss: 0.0201 \n",
            "\n",
            "Epoch 17/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:41 s \tLearning rate: 0.00500 \tTrain class loss: 0.1046 \tTrain src domain loss: 0.0383 \tTrain trg domain loss: 0.0235 \n",
            "\n",
            "Epoch 18/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:43 s \tLearning rate: 0.00500 \tTrain class loss: 0.0963 \tTrain src domain loss: 0.0295 \tTrain trg domain loss: 0.0300 \n",
            "\n",
            "Epoch 19/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:43 s \tLearning rate: 0.00500 \tTrain class loss: 0.0955 \tTrain src domain loss: 0.0331 \tTrain trg domain loss: 0.0208 \n",
            "\n",
            "Epoch 20/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:43 s \tLearning rate: 0.00500 \tTrain class loss: 0.0749 \tTrain src domain loss: 0.0251 \tTrain trg domain loss: 0.0184 \n",
            "\n",
            "Epoch 21/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:45 s \tLearning rate: 0.00050 \tTrain class loss: 0.0747 \tTrain src domain loss: 0.0248 \tTrain trg domain loss: 0.0254 \n",
            "\n",
            "Epoch 22/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:42 s \tLearning rate: 0.00050 \tTrain class loss: 0.0689 \tTrain src domain loss: 0.0230 \tTrain trg domain loss: 0.0156 \n",
            "\n",
            "Epoch 23/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:44 s \tLearning rate: 0.00050 \tTrain class loss: 0.0639 \tTrain src domain loss: 0.0301 \tTrain trg domain loss: 0.0176 \n",
            "\n",
            "Epoch 24/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:42 s \tLearning rate: 0.00050 \tTrain class loss: 0.0662 \tTrain src domain loss: 0.0303 \tTrain trg domain loss: 0.0229 \n",
            "\n",
            "Epoch 25/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:42 s \tLearning rate: 0.00050 \tTrain class loss: 0.0549 \tTrain src domain loss: 0.0264 \tTrain trg domain loss: 0.0228 \n",
            "\n",
            "Epoch 26/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:43 s \tLearning rate: 0.00050 \tTrain class loss: 0.0599 \tTrain src domain loss: 0.0263 \tTrain trg domain loss: 0.0188 \n",
            "\n",
            "Epoch 27/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:43 s \tLearning rate: 0.00050 \tTrain class loss: 0.0611 \tTrain src domain loss: 0.0242 \tTrain trg domain loss: 0.0224 \n",
            "\n",
            "Epoch 28/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:42 s \tLearning rate: 0.00050 \tTrain class loss: 0.0709 \tTrain src domain loss: 0.0298 \tTrain trg domain loss: 0.0181 \n",
            "\n",
            "Epoch 29/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:42 s \tLearning rate: 0.00050 \tTrain class loss: 0.0592 \tTrain src domain loss: 0.0263 \tTrain trg domain loss: 0.0190 \n",
            "\n",
            "Epoch 30/30:\n",
            "Progress: |██████████|  100% \tTime: 00:01:43 s \tLearning rate: 0.00050 \tTrain class loss: 0.0587 \tTrain src domain loss: 0.0281 \tTrain trg domain loss: 0.0240 \n",
            "\n",
            "Total elapsed time: 00:51:59 s\n"
          ]
        }
      ],
      "source": [
        "model = get_model(device)\n",
        "\n",
        "os.makedirs(f\"{res_dir}/res\", exist_ok=True)\n",
        "\n",
        "dir_name = f\"{model.__class__.__name__}_{VERSION}\"\n",
        "for file in os.listdir(f\"{res_dir}/res\"):\n",
        "    if file == dir_name:\n",
        "        raise Exception(f\"Directory {dir_name} already exists\")\n",
        "\n",
        "base_dir = f\"{res_dir}/res/{dir_name}\"\n",
        "sub_dirs = [base_dir, f\"{base_dir}/weights\", f\"{base_dir}/plots\"]\n",
        "for sub_dir in sub_dirs:\n",
        "    os.makedirs(sub_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Training\n",
        "train_monitor = Monitor(file_name=f\"{base_dir}/training_log.txt\")\n",
        "\n",
        "loss_function = get_loss_function()\n",
        "optimizer = get_optimizer(model)\n",
        "scheduler = get_scheduler(optimizer)\n",
        "\n",
        "train_monitor.log(f\"Model:\\n{model}\\n\")\n",
        "train_monitor.log(f\"Loss function:\\n{loss_function}\\n\")\n",
        "train_monitor.log(f\"Optimizer:\\n{optimizer}\\n\")\n",
        "train_monitor.log(f\"Scheduler:\\n{scheduler.__class__.__name__}\")\n",
        "for attr in dir(scheduler):\n",
        "    if not attr.startswith(\"_\") and not callable(getattr(scheduler, attr)):\n",
        "        train_monitor.log(f\"{attr}: {getattr(scheduler, attr)}\")\n",
        "train_monitor.log(\"\\n\")\n",
        "\n",
        "train_dann_rev(\n",
        "    model=model,\n",
        "    trainloader=trainloader,\n",
        "    loss_function=loss_function,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    device=device,\n",
        "    monitor=train_monitor,\n",
        "    base_dir=base_dir\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ca6a5f82",
      "metadata": {
        "id": "ca6a5f82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1702615/2677545298.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(file_name, map_location=torch.device(device)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing model: last.pt\n",
            "Testing dataset: traindataset (cartoon)\n",
            "Testing:\n",
            "Progress: |██████████|  100% \tTime: 00:00:13 s \tTest accuracy: 0.9987 \n",
            "\n",
            "Accuracy on test images: 99.870 %\n",
            "Mean inference time: 5.245 ms\n",
            "Standard deviation of inference time: 0.934 ms\n",
            "\n",
            "\n",
            "Testing dataset: testdataset (sketch)\n",
            "Testing:\n",
            "Progress: |██████████|  100% \tTime: 00:00:24 s \tTest accuracy: 0.5961 \n",
            "\n",
            "Accuracy on test images: 59.608 %\n",
            "Mean inference time: 5.908 ms\n",
            "Standard deviation of inference time: 0.845 ms\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"last.pt\"\n",
        "\n",
        "model = get_model(device)\n",
        "\n",
        "dir_name = f\"{model.__class__.__name__}_{VERSION}\"\n",
        "base_dir = f\"{res_dir}/res/{dir_name}\"\n",
        "\n",
        "# Testing\n",
        "test_monitor = Monitor(file_name=f\"{base_dir}/testing_log.txt\")\n",
        "model = load_model(model, f\"{base_dir}/weights/{MODEL_NAME}\", device)\n",
        "\n",
        "test_monitor.log(f\"Testing model: {MODEL_NAME}\")\n",
        "\n",
        "test_monitor.log(f\"Testing dataset: traindataset (cartoon)\")\n",
        "\n",
        "test(\n",
        "    model,\n",
        "    trainloader,\n",
        "    device,\n",
        "    test_monitor\n",
        ")\n",
        "\n",
        "test_monitor.log(f\"\\n\")\n",
        "test_monitor.log(f\"Testing dataset: testdataset (sketch)\")\n",
        "\n",
        "test(\n",
        "    model,\n",
        "    testloader,\n",
        "    device,\n",
        "    test_monitor\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
